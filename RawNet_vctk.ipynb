{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statutory-thong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cce: 2.054:   3%|▌                   | 13/428 [00:04<02:02,  3.39it/s]^C\n",
      "epoch: 1, cce: 2.054:   3%|▌                   | 13/428 [00:04<02:17,  3.02it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"main2.py\", line 158, in <module>\n",
      "    loss.backward()\n",
      "  File \"/scratch/ssd001/home/marko/thesis/torch/tensor.py\", line 221, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/scratch/ssd001/home/marko/thesis/torch/autograd/__init__.py\", line 132, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python main2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structural-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os, pickle\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "from dataloader import *\n",
    "import sys\n",
    "DIR = '/h/marko/CSC413/RawNet/python/RawNet2/Pre-trained_model'\n",
    "sys.path.append( DIR )\n",
    "from model_RawNet2_original_code import RawNet as RawNet2\n",
    "\n",
    "from parser import get_args\n",
    "from trainer import *\n",
    "from utils import *\n",
    "\n",
    "from argparse import Namespace\n",
    "import glob, json, argparse\n",
    "\n",
    "MAGIC_NUMBER = 59049 # previously 66150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "durable-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = Namespace(**{\n",
    "    \"bs\": 25,\n",
    "    \"lr\": 0.001,\n",
    "    \"nb_samp\": MAGIC_NUMBER,\n",
    "    \"name\": 'vctk-exp-16',\n",
    "    \"save_dir\": 'DNNs/',\n",
    "    \"DB\": '/',\n",
    "    \"window_size\": 0,\n",
    "    \"wd\": 0.001,\n",
    "    \"epoch\": 60,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"nb_worker\": 4,\n",
    "    \"temp\": .5,\n",
    "    \"seed\": 12315,\n",
    "    \"load_model_dir\": '/h/marko/CSC413/RawNet/python/RawNet2/Pre-trained_model/rawnet2_best_weights.pt',\n",
    "    \"m_first_conv\": 251,\n",
    "    \"m_in_channels\": 1,\n",
    "    \"m_filts\": [128, [128,128], [128,256], [256,256]],\n",
    "    \"m_blocks\": [2, 4],\n",
    "    \"m_nb_fc_att_node\": [1],\n",
    "    \"m_nb_fc_node\": 1024,\n",
    "    \"m_gru_node\": 1024,\n",
    "    \"m_nb_gru_layer\": 1,\n",
    "    \"m_nb_samp\": MAGIC_NUMBER,\n",
    "    \"amsgrad\": True,\n",
    "    \"make_val_trial\": False,\n",
    "    \"debug\": False,\n",
    "    \"comet_disable\": False,\n",
    "    \"save_best_only\": False,\n",
    "    \"mg\": False,\n",
    "    \"load_model\": True,\n",
    "    \"reproducible\": True,\n",
    "})\n",
    "args.model = {}\n",
    "for k, v in vars(args).items():\n",
    "    if k[:2] == 'm_':\n",
    "        # print(k, v)\n",
    "        args.model[k[2:]] = v\n",
    "args.model['nb_classes'] = 6112"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-edward",
   "metadata": {},
   "source": [
    "### Train set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hungry-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultispeakerDataset(data.Dataset):\n",
    "    def __init__(self, index, path):\n",
    "        self.path = path\n",
    "        self.index = index\n",
    "        self.all_files = [(i, name) for (i, speaker) in enumerate(index) for name in speaker]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        speaker_id, name = self.all_files[index]\n",
    "        speaker_onehot = (np.arange(len(self.index)) == speaker_id).astype(np.long)\n",
    "        audio = np.load(f'{self.path}/{speaker_id}/{name}.npy')\n",
    "        \n",
    "        audio = (audio / np.abs(audio.max())).astype(np.float32)\n",
    "        if audio.shape[0] < MAGIC_NUMBER:\n",
    "            audio = np.tile(audio, int(np.ceil(MAGIC_NUMBER/audio.shape[0])))\n",
    "#         audio = np.abs(librosa.stft(audio[0:MAGIC_NUMBER], n_fft=1024, window=scipy.signal.hanning, hop_length=512))[:,:128]\n",
    "        \n",
    "        return audio[0:MAGIC_NUMBER], speaker_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def num_speakers(self):\n",
    "        return len(self.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baking-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/h/marko/vctk/WaveRnn-Multispeaker'\n",
    "with open(f'{data_path}/index.pkl', 'rb') as f:\n",
    "    index = pickle.load(f)\n",
    "\n",
    "train_index = [[_ for o,_ in enumerate(x) if o % 10 != 0] for i, x in enumerate(index) if i < 16]\n",
    "val_index = [x[::10] for i, x in enumerate(index) if i < 16]\n",
    "\n",
    "trainset = MultispeakerDataset(train_index, data_path)\n",
    "trainset_gen = data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size = args.bs,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = args.nb_worker)\n",
    "\n",
    "valset = MultispeakerDataset(val_index, data_path)\n",
    "valset_gen = data.DataLoader(\n",
    "    valset,\n",
    "    batch_size = args.bs,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = args.nb_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "capital-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set save directory\n",
    "save_dir = args.save_dir + args.name + '/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(save_dir+'results/', exist_ok=True)\n",
    "os.makedirs(save_dir+'models/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acoustic-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RawNet2(args.model, 'cuda').to('cuda')\n",
    "if args.load_model: model.load_state_dict(torch.load(args.load_model_dir))\n",
    "nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subjective-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc2_gru = nn.Linear(in_features = args.model['nb_fc_node'],\n",
    "    out_features = 16,\n",
    "    bias = True)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optional-offering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13379378"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "studied-circumstances",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f6b639dd22c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     {\n\u001b[1;32m      8\u001b[0m         'params': [\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mparam\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'bn'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#set ojbective funtions\n",
    "criterion = {}\n",
    "criterion['cce'] = nn.CrossEntropyLoss()\n",
    "\n",
    "#set optimizer\n",
    "params = [\n",
    "    {\n",
    "        'params': [\n",
    "            param for name, param in model.named_parameters()\n",
    "            if 'bn' not in name\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'params': [\n",
    "            param for name, param in model.named_parameters()\n",
    "            if 'bn' in name\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params,\n",
    "    lr = args.lr,\n",
    "    weight_decay = args.wd,\n",
    "    amsgrad = args.amsgrad)\n",
    "# if args.load_model: optimizer.load_state_dict(torch.load(args.load_model_opt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "level-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda step: keras_lr_decay(step))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-delicious",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "direct-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    global valset, valset_gen\n",
    "    corr = 0\n",
    "    n = len(valset)\n",
    "    for m_batch, m_label in tqdm(valset_gen):\n",
    "        m_batch, m_label = m_batch.cuda(), m_label.cuda()\n",
    "        output = model(m_batch, m_label)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        corr += (m_label == pred).sum().item()\n",
    "    print(f'accuracy: {corr*100/n:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "satisfied-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:06<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 93.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verbal-request",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RawNet(\n",
       "  (ln): LayerNorm()\n",
       "  (first_conv): SincConv_fast()\n",
       "  (first_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "  (block0): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block1): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv_downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (lrelu_keras): LeakyReLU(negative_slope=0.3)\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (mp): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc_attention0): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc_attention1): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc_attention2): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention3): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention4): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_attention5): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (bn_before_gru): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (gru): GRU(256, 1024, batch_first=True)\n",
       "  (fc1_gru): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (fc2_gru): Linear(in_features=1024, out_features=16, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RawNet2(args.model, 'cuda')\n",
    "model.fc2_gru = nn.Linear(in_features = args.model['nb_fc_node'],\n",
    "    out_features = 16,\n",
    "    bias = True)\n",
    "model.load_state_dict(torch.load(save_dir +  f'models/TA_40.pt'))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-switch",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-taste",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 0.608, acc: 20.02: 100%|██████████| 428/428 [03:01<00:00,  2.35it/s]\n",
      "epoch: 2, avg loss: 0.259, acc: 22.83: 100%|██████████| 428/428 [03:01<00:00,  2.36it/s]\n",
      "epoch: 3, avg loss: 0.191, acc: 23.51: 100%|██████████| 428/428 [02:59<00:00,  2.38it/s]\n",
      "epoch: 4, avg loss: 0.175, acc: 23.58: 100%|██████████| 428/428 [02:58<00:00,  2.40it/s]\n",
      "epoch: 5, avg loss: 0.139, acc: 23.85: 100%|██████████| 428/428 [03:03<00:00,  2.33it/s]\n",
      "epoch: 6, avg loss: 0.124, acc: 24.00: 100%|██████████| 428/428 [03:05<00:00,  2.31it/s]\n",
      "epoch: 7, avg loss: 0.115, acc: 24.04: 100%|██████████| 428/428 [02:59<00:00,  2.38it/s]\n",
      "epoch: 8, avg loss: 0.079, acc: 24.44: 100%|██████████| 428/428 [03:01<00:00,  2.36it/s]\n",
      "epoch: 9, avg loss: 0.097, acc: 24.23: 100%|██████████| 428/428 [02:59<00:00,  2.38it/s]\n",
      "epoch: 10, avg loss: 0.073, acc: 24.43: 100%|██████████| 428/428 [03:31<00:00,  2.03it/s]\n",
      "epoch: 11, avg loss: 0.057, acc: 24.52: 100%|██████████| 428/428 [02:56<00:00,  2.43it/s]\n",
      "epoch: 12, avg loss: 0.075, acc: 24.43: 100%|██████████| 428/428 [03:00<00:00,  2.37it/s]\n",
      "epoch: 13, avg loss: 0.077, acc: 24.38: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 14, avg loss: 0.060, acc: 24.50: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 15, avg loss: 0.056, acc: 24.57: 100%|██████████| 428/428 [02:56<00:00,  2.42it/s]\n",
      "epoch: 16, avg loss: 0.062, acc: 24.51: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 17, avg loss: 0.040, acc: 24.72: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 18, avg loss: 0.039, acc: 24.68: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 19, avg loss: 0.054, acc: 24.59: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 20, avg loss: 0.056, acc: 24.58: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 21, avg loss: 0.037, acc: 24.71: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 22, avg loss: 0.031, acc: 24.78: 100%|██████████| 428/428 [02:54<00:00,  2.45it/s]\n",
      "epoch: 23, avg loss: 0.042, acc: 24.68: 100%|██████████| 428/428 [02:54<00:00,  2.46it/s]\n",
      "epoch: 24, cce: 0.013:  38%|███▊      | 161/428 [01:05<01:48,  2.46it/s]"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "for epoch in range(args.epoch):\n",
    "    model.train()\n",
    "    corr = 0\n",
    "    with tqdm(total = len(trainset_gen)+1, leave=True) as pbar:\n",
    "        epoch_loss = 0\n",
    "        for m_batch, m_label in trainset_gen:\n",
    "            m_batch, m_label = m_batch.to(device), m_label.to(device)\n",
    "            output = model(m_batch, m_label)\n",
    "            cce_loss = criterion['cce'](output, m_label)\n",
    "            loss = cce_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            _, pred = torch.max(output, 1)\n",
    "            corr += (m_label == pred).sum().item()\n",
    "            \n",
    "            pbar.set_description(f'epoch: {epoch+1}, cce: {cce_loss:.3f}')\n",
    "            pbar.update(1)\n",
    "                    \n",
    "        epoch_loss /= len(trainset_gen)\n",
    "        pbar.set_description(f'epoch: {epoch+1}, avg loss: {epoch_loss:.3f}, acc: {corr/len(trainset_gen):.2f}')\n",
    "        pbar.update(1)\n",
    "                    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), save_dir +  f'models/TA_{epoch+1}.pt')\n",
    "        torch.save(optimizer.state_dict(), save_dir + 'models/best_opt_eval.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-toyota",
   "metadata": {},
   "source": [
    "### Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(parametrization={}):\n",
    "    args = parametrization\n",
    "    myIndex = open_dir('index')\n",
    "    myQueryParser = QueryParser(\"file_content\", schema=myIndex.schema, group=qparser.OrGroup)\n",
    "    mySearcher = myIndex.searcher(weighting=BM25F(B=args.get('B', 0.524), K1=args.get('K1', 3)))\n",
    "    res = pyTrecEval(TOPIC_FILE, QRELS_FILE, myQueryParser, mySearcher)\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
